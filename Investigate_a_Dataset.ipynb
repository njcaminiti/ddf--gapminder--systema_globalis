{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the Gapminder Systema Globalis Dataset\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "The name \"Systema Globalis\" is inspired by [Systema Naturae](https://en.wikipedia.org/wiki/Systema_Naturae); the first systematic compilation of all living things from year 1735, by the Swedish Scientist Carl von Linn√©. The goal of Systema Globalis is to compile all public statistics; Social, Economic and Environmental; into a comparable total dataset.\n",
    "\n",
    "### Data\n",
    "This is the main dataset used in tools on the official Gapminder website. It contains local &amp; global statistics combined from hundreds of sources.\n",
    "\n",
    "For the purposes of this demonstration, we will be using the glob module for file manipulation, pandas and numpy for working with the data, and matplotlib's basic pyplot suite for visualization.\n",
    "\n",
    "_This exercise is intended to illustrate the choices an analyst must make at each point of the data analysis process, from the initial data-wrangling to the final presentation of findings. The process shown is not being presented as an optimal route. In fact, there will be points along the way where less than optimal choices are made. The intent is to illustrate the potential pitfalls an analyst will encounter when working with an unknown dataset, as well as to demonstrate the shortcomings of certain decisions the novice analyst is likely to make. At these points, suggestions as to how the process could have been improved will be provided and/or the process will be revised and re-performed._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling\n",
    "\n",
    "The Systema Globalis data consists of longitudinal country-level data on over 500 indicators of social, economic, health, environmental, and other comparative measures. The data on each indicator is contained in its own csv file, organized by country (geo) and year of measurement (time). \n",
    "\n",
    "### General Properties\n",
    "\n",
    "Due to the multi-file structure of this dataset, analysis involving more than one indicator requires file operations to parse, read, and merge the data from each target indicator. The glob module simplifies this process, allowing for filename 'globbing' based on wildcard searches of filenames. This provides one method of subsetting the files and extracting only the data we are interested in analyzing. \n",
    "\n",
    "For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ddf--datapoints--literacy_rate_adult_female_percent_of_females_ages_15_above--by--geo--time.csv'\n",
      " 'ddf--datapoints--literacy_rate_adult_male_percent_of_males_ages_15_and_above--by--geo--time.csv'\n",
      " 'ddf--datapoints--literacy_rate_adult_total_percent_of_people_ages_15_and_above--by--geo--time.csv'\n",
      " 'ddf--datapoints--literacy_rate_youth_female_percent_of_females_ages_15_24--by--geo--time.csv'\n",
      " 'ddf--datapoints--literacy_rate_youth_male_percent_of_males_ages_15_24--by--geo--time.csv'] \n",
      "\n",
      "\n",
      "['ddf--datapoints--energy_from_solid_biofuels_percent--by--geo--time.csv'\n",
      " 'ddf--datapoints--energy_production_per_person--by--geo--time.csv'\n",
      " 'ddf--datapoints--energy_production_total--by--geo--time.csv'\n",
      " 'ddf--datapoints--energy_supply_per_person_toe--by--geo--time.csv'\n",
      " 'ddf--datapoints--energy_use_per_person--by--geo--time.csv']\n"
     ]
    }
   ],
   "source": [
    "# Returns on the filenames of 'datapoint' csv files containing literacy data:\n",
    "datafiles_literacy = glob.glob('*datapoints*literacy*.csv')\n",
    "print(np.array(datafiles_literacy)[:5], '\\n\\n')\n",
    "\n",
    "# Returns only the filenames of 'datapoint' csv files containing energy data:\n",
    "datafiles_energy = glob.glob('*datapoints*energy*.csv')\n",
    "print(np.array(datafiles_energy)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the alternative, we could try reading all of the data into Python, and holding off on trimming it down or subsetting it until after its already been processed.  Due to the size and complexity of the dataset, this will be more computationally intensive than importing only a target subset, but there is a certain appeal to compiling all of the relevant data in a single place, so lets give that a try.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles_all = glob.glob('*datapoints*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gapminder data is particularly well suited to exploratory data analysis because it is largely 'pre-cleaned'. Each table shares a uniform format, making it very easy to read in and merge the data from multiple indicators. \n",
    "\n",
    "While data 'wrangling' can be a complex, time-consuming process, especially when dealing with messy data or data from multiple sources, it can be accomplished here with relative ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty dictionary into which we will read all of our data files\n",
    "ref = {}\n",
    "\n",
    "# loop through filenames, extracting indicator name from filename, and values from each file itself\n",
    "for filename in datafiles_all:\n",
    "    key = filename.partition('datapoints--')[2].partition('--')[0]\n",
    "    data = pd.read_csv(filename)\n",
    "    # index each table by country and cast longitudinal progression to columns \n",
    "    data = data.pivot(index='geo', columns='time')\n",
    "    # incorporate second-level index (indicator name), necessary when merging multiple indicators \n",
    "    data = data.stack(level=0)\n",
    "    data.index.set_names('indicator', level=1, inplace=True)\n",
    "    # dictionary entry: key = indicator name, value = reformatted dataframe\n",
    "    ref[key] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a reference dictionary containing the entire gapminder dataset. As mentioned eariler, this \"everything in one place\" approach is sub-optimal for a number of reasons, one of which will become apparent soon. \n",
    "\n",
    "We will finish the process by concatenating all of these individual tables into a single master dataframe. Again, we ensured that this would go smoothly by reindexing and reshaping in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([1086, 1100, 1279, 1290, 1300, 1309, 1348, 1349, 1351, 1352,\n",
      "            ...\n",
      "            2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100],\n",
      "           dtype='int64', name='time', length=409)\n",
      "['geo', 'indicator']\n",
      "33046225\n",
      "2899967\n"
     ]
    }
   ],
   "source": [
    "master = pd.concat(ref.values())\n",
    "master = master.sort_index()\n",
    "# let's take a look at some attributes of our master dataframe\n",
    "print(master.columns)\n",
    "print(master.index.names)\n",
    "print(master.isna().sum().sum())\n",
    "print(master.count().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Whoa__, this is a bit of a mess. There are datapoints going back all the way to the 11th century, and there are even points out into the future! How is that possible? \n",
    "\n",
    "Furthermore, our table contains over 33 million missing values, and fewer than 3 million actual values. In other words, the majority of our dataframe is pretty much useless.\n",
    "\n",
    "A closer look at our indicators shows that while most of the indicators contain observed values, some are projections. This explains the values for future years. We could fix this by not loading in those particular indicators, or by filtering our data to a specific timeframe. \n",
    "\n",
    "How about those missing values though?  Let's take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "1086       1\n",
      "1100       1\n",
      "1279       1\n",
      "1290       1\n",
      "1300       1\n",
      "1309       2\n",
      "1348       1\n",
      "1349       1\n",
      "1351       1\n",
      "1352       2\n",
      "1365       2\n",
      "1374       2\n",
      "1377       1\n",
      "1409       2\n",
      "1413       1\n",
      "1428       2\n",
      "1450       1\n",
      "1459       2\n",
      "1489       1\n",
      "1500       7\n",
      "1545       1\n",
      "1555       1\n",
      "1570      11\n",
      "1575       1\n",
      "1586       1\n",
      "1590       1\n",
      "1600      85\n",
      "1609      21\n",
      "1610       1\n",
      "1614       1\n",
      "        ... \n",
      "2071    1830\n",
      "2072    1830\n",
      "2073    1830\n",
      "2074    1830\n",
      "2075    8630\n",
      "2076    1830\n",
      "2077    1830\n",
      "2078    1830\n",
      "2079    1830\n",
      "2080    8830\n",
      "2081    1830\n",
      "2082    1830\n",
      "2083    1830\n",
      "2084    1830\n",
      "2085    8630\n",
      "2086    1830\n",
      "2087    1830\n",
      "2088    1830\n",
      "2089    1830\n",
      "2090    8630\n",
      "2091    1830\n",
      "2092    1830\n",
      "2093    1830\n",
      "2094    1830\n",
      "2095    8630\n",
      "2096    1830\n",
      "2097    1830\n",
      "2098    1830\n",
      "2099    2416\n",
      "2100    8060\n",
      "Length: 409, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(master.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that explains a lot. If there were no values missing in our table, we would expect to see almost 88,000 observations for each year. Here we're seeing a bunch of years with only 2000 or even just one single value that's a LOT of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59140\n"
     ]
    }
   ],
   "source": [
    "print(master.count().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even our most populated column contains almost 30,000 missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "In our attempt to put everything in one place, we created a HUGE but mostly empty dataset. The most obvious culprits based on our cursory look are the very early years that contain very few observations, and the future (projection) years which also contain relatively few observations. \n",
    "\n",
    "There are some less obvious complications though:\n",
    "1) Countries whose names change\n",
    "2) Countries for which very little data has been collected\n",
    "3) Indicators which only contain data for a limited timeframe\n",
    "\n",
    "These can each be addressed, whether by \n",
    "1) limiting the scope of analysis to only certain countries, \n",
    "2) certain indicators, or \n",
    "3) certain time-periods \n",
    "\n",
    "Each of these solutions moves us away from the 'everything in one place' approach we were initially going for, but it seems like with this dataset it just isn't practical to mash everything together like that. We'll start over and create two new datasets, each applying a hybrid of the filtering methods above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering by time and indicator \n",
    "d1 = {}\n",
    "\n",
    "for filename in datafiles_all:\n",
    "    if re.search('income|cancer.deaths.*100000', filename):\n",
    "        key = filename.partition('datapoints--')[2].partition('--')[0]\n",
    "        data = pd.read_csv(filename)\n",
    "        # 'pre-cleaning' step to exclude data from earlier than 1950, and exclude projections\n",
    "        data = data[data.time >= 1950]\n",
    "        data = data[data.time <= 2018] \n",
    "        data = data.pivot(index='geo', columns='time')\n",
    "        data = data.stack(level=0)\n",
    "        data.index.set_names('indicator', level=1, inplace=True)\n",
    "        d1[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering by time and country\n",
    "d2 = {}\n",
    "\n",
    "for filename in datafiles_all:\n",
    "    key = filename.partition('datapoints--')[2].partition('--')[0]\n",
    "    data = pd.read_csv(filename)\n",
    "    data = data[data.time >= 1950]\n",
    "    data = data[data.time <= 2018] \n",
    "    data = data.loc[(data.geo == 'usa') | (data.geo == 'ger') | (data.geo == 'chi') |\n",
    "                    (data.geo == 'jap') | (data.geo == 'aus')]\n",
    "    data = data.pivot(index='geo', columns='time')\n",
    "    data = data.stack(level=0)\n",
    "    data.index.set_names('indicator', level=1, inplace=True)\n",
    "    d2[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-c95bfc2a2c51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    210\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                        copy=copy)\n\u001b[0m\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "t1 = pd.concat(d1.values())\n",
    "t1 = t1.sort_index()\n",
    "print(t1.shape)\n",
    "print(t1.isna().sum().sum())\n",
    "print(t1.count().sum())\n",
    "\n",
    "t2 = pd.concat(d2.values())\n",
    "t2 = t2.sort_index()\n",
    "print(t2.shape)\n",
    "print(t2.isna().sum().sum())\n",
    "print(t2.count().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "> **Tip**: Now that you've trimmed and cleaned your data, you're ready to move on to exploration. Compute statistics and create visualizations with the goal of addressing the research questions that you posed in the Introduction section. It is recommended that you be systematic with your approach. Look at one variable at a time, and then follow it up by looking at relationships between variables.\n",
    "\n",
    "### Research Question 1 (Replace this header name!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this, and more code cells, to explore your data. Don't forget to add\n",
    "#   Markdown cells to document your observations and findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 2  (Replace this header name!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue to explore the data to address your additional research\n",
    "#   questions. Add more headers as needed if you have more questions to\n",
    "#   investigate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "\n",
    "> **Tip**: Finally, summarize your findings and the results that have been performed. Make sure that you are clear with regards to the limitations of your exploration. If you haven't done any statistical tests, do not imply any statistical conclusions. And make sure you avoid implying causation from correlation!\n",
    "\n",
    "> **Tip**: Once you are satisfied with your work here, check over your report to make sure that it is satisfies all the areas of the rubric (found on the project submission page at the end of the lesson). You should also probably remove all of the \"Tips\" like this one so that the presentation is as polished as possible.\n",
    "\n",
    "## Submitting your Project \n",
    "\n",
    "> Before you submit your project, you need to create a .html or .pdf version of this notebook in the workspace here. To do that, run the code cell below. If it worked correctly, you should get a return code of 0, and you should see the generated .html file in the workspace directory (click on the orange Jupyter icon in the upper left).\n",
    "\n",
    "> Alternatively, you can download this report as .html via the **File** > **Download as** submenu, and then manually upload it into the workspace directory by clicking on the orange Jupyter icon in the upper left, then using the Upload button.\n",
    "\n",
    "> Once you've done this, you can submit your project by clicking on the \"Submit Project\" button in the lower right here. This will create and submit a zip file with this .ipynb doc and the .html or .pdf version you created. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Investigate_a_Dataset.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
